{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "> ğŸŒŸ å¦‚æœä½ è§‰å¾— ChatTTS å’Œ ChatTTS_colab é¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·è®¿é—®ä»¥ä¸‹é“¾æ¥ç»™å®ƒä»¬ç‚¹ä¸ªæ˜Ÿæ˜Ÿå§ï¼ğŸŒŸ\n",
        "\n",
        "- [ChatTTS é¡¹ç›®](https://github.com/2noise/ChatTTS)\n",
        "\n",
        "- [ChatTTS_colab é¡¹ç›®](https://github.com/6drf21e/ChatTTS_colab)\n",
        "\n",
        "æ„Ÿè°¢ä½ çš„æ”¯æŒï¼\n",
        "\n",
        "# è¿è¡Œæ–¹æ³•\n",
        "\n",
        "- ç‚¹å‡»èœå•æ çš„--ä»£ç æ‰§è¡Œç¨‹åº--å…¨éƒ¨è¿è¡Œå³å¯\n",
        "- æ‰§è¡Œååœ¨ä¸‹æ–¹çš„æ—¥å¿—ä¸­æ‰¾åˆ°ç±»ä¼¼\n",
        "\n",
        "  Running on public URL: https://**************.gradio.live  <-è¿™ä¸ªå°±æ˜¯å¯ä»¥è®¿é—®çš„å…¬ç½‘åœ°å€\n",
        "\n",
        "å®‰è£…åŒ…çš„æ—¶å€™æç¤ºè¦é‡å¯ è¯·ç‚¹**\"å¦\"**"
      ],
      "metadata": {
        "id": "Xo3k5XsTzWK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -q https://github.com/6drf21e/ChatTTS_colab\n",
        "%cd ChatTTS_colab\n",
        "!git clone -q https://github.com/2noise/ChatTTS\n",
        "%cd ChatTTS\n",
        "!git checkout -q 4c201cd\n",
        "%cd ..\n",
        "!mv ChatTTS abc\n",
        "!mv abc/* /content/ChatTTS_colab/\n",
        "!pip install -q omegaconf vocos vector_quantize_pytorch gradio cn2an pypinyin openai jieba WeTextProcessing python-dotenv\n",
        "# å¯åŠ¨ Gradio æœ‰å…¬ç½‘åœ°å€\n",
        "!python webui_mix.py --share\n"
      ],
      "metadata": {
        "id": "hNDl-5muR77-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa74fde8-234e-42de-cbe9-95c1b27404e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ChatTTS_colab' already exists and is not an empty directory.\n",
            "/content/ChatTTS_colab\n",
            "fatal: destination path 'ChatTTS' already exists and is not an empty directory.\n",
            "/content/ChatTTS_colab/ChatTTS\n",
            "error: pathspec '651093' did not match any file(s) known to git\n",
            "/content/ChatTTS_colab\n",
            "2025-05-23 09:54:49.757816: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747994089.780162    4470 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747994089.786434    4470 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-23 09:54:49.809763: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading ChatTTS model...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ChatTTS_colab/webui_mix.py\", line 46, in <module>\n",
            "    chat = load_chat_tts_model(source=args.source, local_path=args.local_path)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/tts_model.py\", line 25, in load_chat_tts_model\n",
            "    chat.load_models(source=source, force_redownload=force_redownload, custom_path=local_path, compile=False)\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/core.py\", line 81, in load_models\n",
            "    self._load(**{k: os.path.join(download_path, v) for k, v in OmegaConf.load(os.path.join(download_path, 'config', 'path.yaml')).items()}, **kwargs)\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/core.py\", line 144, in _load\n",
            "    tokenizer = torch.load(tokenizer_path, map_location='cpu')\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 1470, in load\n",
            "    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n",
            "_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
            "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
            "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
            "\tWeightsUnpickler error: Unsupported global: GLOBAL transformers.models.bert.tokenization_bert_fast.BertTokenizerFast was not an allowed global by default. Please use `torch.serialization.add_safe_globals([BertTokenizerFast])` or the `torch.serialization.safe_globals([BertTokenizerFast])` context manager to allowlist this global if you trust this class/function.\n",
            "\n",
            "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n"
          ]
        }
      ]
    }
  ]
}